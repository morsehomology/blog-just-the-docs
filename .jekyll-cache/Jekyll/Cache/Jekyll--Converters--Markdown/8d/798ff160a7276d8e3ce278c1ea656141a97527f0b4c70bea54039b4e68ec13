I"˝<!--  -->

<div class="t">Generalization and Gradient Descent</div>

<!-- <details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details> -->

<!-- --- -->

<h1 id="1-bias-variance-exposition">1. ‚ÄúBias Variance‚Äù exposition</h1>

<p>Daniela Witten says the tradeoff is related to the equation</p>

\[\text{Exp. Pred. Error }=\text{ Irreducible Error + Bias}^2+ \text{Var}\]

<p>There will be different frameworks for this, we‚Äôre going to try to start with as little structure as possible if we can.</p>

<h2 id="11-versionattempt-1">1.1. Version/Attempt 1</h2>

<p>(I have reasons to think this version can‚Äôt be correct, doesn‚Äôt have some features of the problem which seemed essential. We want to see that though).</p>

<p>Have a space $X$ and the data is determined by a distribution $\rho$ on $X$. A model (fit) is another distribution $\rho‚Äô$ on $X$. For now we meed to assume that $X$ has a fixed metric $d$ and furthermore that means/centers of mass are defined for $\rho$ and $\rho‚Äô$, call these $x_{\rho}$ and $x_{\rho‚Äô}$ in $X$.</p>

<p>(They should be defined by minimizing $E_{\rho}[d(x_{\rho}, x)]$ over all of $X$, and likewise for $\rho‚Äô$).</p>

<p>The variance of</p>

<h1 id="2-from-ben-recht-slides">2. From Ben Recht slides</h1>

<p><strong>Given</strong>: i.i.d. sample \(S=\left\{z_1, \ldots, z_n\right\}\) from distribution $D$<br />
<strong>Goal</strong>: Find a good predictor function $f$<br />
Population risk (test error): \(R[f]=\mathbb{E}_z \operatorname{loss}(f ; z)\) Unknown!<br />
Empirical risk (training error): \(R_s[f]=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(f ; z_i\right)\) Minimize using SGD!<br />
Generalization error: $R[f]-R_s[f]$</p>

<p>How much empirical risk underestimates population risk</p>

<p>We can compute $R_s \ldots$:</p>

<p>‚ÄúFundamental theorem of ML:‚Äù</p>

\[R[f]=\left(R[f]-R_S[f]\right)+R_S[f]\]

<p>population generalization training - small training error implies risk ¬± generalization error - zero training error does not imply overfitting \(\begin{aligned}
R[f] &amp;=\left(R[f]-R\left[f_{\mathcal{H}}\right]\right) \\
&amp;+\left(R\left[f_{\mathcal{H}}\right]-R\left[f_{\star}\right]\right) \\
&amp;+R\left[f_{\star}\right]
\end{aligned}\)</p>
:ET