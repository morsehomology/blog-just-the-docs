{"0": {
    "doc": "Generalization and Gradient Descent",
    "title": "1. “Bias Variance” exposition",
    "content": "Daniela Witten says the tradeoff is related to the equation . \\[\\text{Exp. Pred. Error }=\\text{ Irreducible Error + Bias}^2+ \\text{Var}\\] There will be different frameworks for this, we’re going to try to start with as little structure as possible if we can. ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/#1-bias-variance-exposition",
    "relUrl": "/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/#1-bias-variance-exposition"
  },"1": {
    "doc": "Generalization and Gradient Descent",
    "title": "1.1. Version/Attempt 1",
    "content": "(I have reasons to think this version can’t be correct, doesn’t have some features of the problem which seemed essential. We want to see that though). Have a space $X$ and the data is determined by a distribution $\\rho$ on $X$. A model (fit) is another distribution $\\rho’$ on $X$. For now we meed to assume that $X$ has a fixed metric $d$ and furthermore that means/centers of mass are defined for $\\rho$ and $\\rho’$, call these $x_{\\rho}$ and $x_{\\rho’}$ in $X$. (They should be defined by minimizing $E_{\\rho}[d(x_{\\rho}, x)]$ over all of $X$, and likewise for $\\rho’$). The variance of . ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/#11-versionattempt-1",
    "relUrl": "/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/#11-versionattempt-1"
  },"2": {
    "doc": "Generalization and Gradient Descent",
    "title": "2. From Ben Recht slides",
    "content": "Given: i.i.d. sample \\(S=\\left\\{z_1, \\ldots, z_n\\right\\}\\) from distribution $D$ Goal: Find a good predictor function $f$ Population risk (test error): \\(R[f]=\\mathbb{E}_z \\operatorname{loss}(f ; z)\\) Unknown! Empirical risk (training error): \\(R_s[f]=\\frac{1}{n} \\sum_{i=1}^n \\operatorname{loss}\\left(f ; z_i\\right)\\) Minimize using SGD! Generalization error: $R[f]-R_s[f]$ . How much empirical risk underestimates population risk . We can compute $R_s \\ldots$: . “Fundamental theorem of ML:” . \\[R[f]=\\left(R[f]-R_S[f]\\right)+R_S[f]\\] population generalization training - small training error implies risk ± generalization error - zero training error does not imply overfitting \\(\\begin{aligned} R[f] &amp;=\\left(R[f]-R\\left[f_{\\mathcal{H}}\\right]\\right) \\\\ &amp;+\\left(R\\left[f_{\\mathcal{H}}\\right]-R\\left[f_{\\star}\\right]\\right) \\\\ &amp;+R\\left[f_{\\star}\\right] \\end{aligned}\\) . ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/#2-from-ben-recht-slides",
    "relUrl": "/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/#2-from-ben-recht-slides"
  },"3": {
    "doc": "Generalization and Gradient Descent",
    "title": "Generalization and Gradient Descent",
    "content": "Generalization and Gradient Descent ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/",
    "relUrl": "/notes/2022-09-02-Generalization%20and%20Gradient%20Descent/"
  },"4": {
    "doc": "Transformers and HPC",
    "title": "1. Background on Transformers",
    "content": " ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-12-Transformers/#1-background-on-transformers",
    "relUrl": "/notes/2022-09-12-Transformers/#1-background-on-transformers"
  },"5": {
    "doc": "Transformers and HPC",
    "title": "1.1. Transformers vs. RNN’s (Motivation 1)",
    "content": "Idea: Framing transformers as an alternative to RNN’s for “sequence-to-sequence” mapping, e.g., for translation . Here, each network will represent a function $f: X \\to X’$, where $X$ and $X’$ denote the spaces of input and output sequences, respectively. More specifically, let $T, T’$ denote the spaces of input and output tokens, and let $I$ denote the index set (need to elaborate on the structure of $I$). Then . \\[X = \\{I \\to T\\}\\] and . \\[X' = \\{I \\to T'\\}\\] Let $M = {X \\to X’}$ denote the space of all sequence-to-sequence mappings. ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-12-Transformers/#11-transformers-vs-rnns-motivation-1",
    "relUrl": "/notes/2022-09-12-Transformers/#11-transformers-vs-rnns-motivation-1"
  },"6": {
    "doc": "Transformers and HPC",
    "title": "1.2. Useful info",
    "content": "1.2.1. Softmax . Softmax function $\\rho: \\R^n \\to \\R^n$, . \\[\\rho\\left(x_i\\right)=\\frac{\\exp \\left(x_i\\right)}{\\sum_j \\exp \\left(x_j\\right)}\\] Possible ways to think about it: . | Define $\\rho_t$ by | . \\[\\rho_t\\left(x_i\\right)=\\frac{\\exp \\left(t x_i\\right)}{\\sum_j \\exp \\left(t x_j\\right)}\\] for $t\\in \\R^+$. ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-12-Transformers/#12-useful-info",
    "relUrl": "/notes/2022-09-12-Transformers/#12-useful-info"
  },"7": {
    "doc": "Transformers and HPC",
    "title": "2. Workflow for AAYN model",
    "content": ". | Inputs come in as a sequence of tokensfdas | fda | . ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-12-Transformers/#2-workflow-for-aayn-model",
    "relUrl": "/notes/2022-09-12-Transformers/#2-workflow-for-aayn-model"
  },"8": {
    "doc": "Transformers and HPC",
    "title": "3. Transformer architectures",
    "content": "References: . What defines a transformer? What are its important properties, how does it differ from other kinds of architectures (and what are the current views of the effects of these differences?) . ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-12-Transformers/#3-transformer-architectures",
    "relUrl": "/notes/2022-09-12-Transformers/#3-transformer-architectures"
  },"9": {
    "doc": "Transformers and HPC",
    "title": "4. Comments",
    "content": "| Comments: | ——— | . | Is there any interesting way to relate some of these transformer vs RNN ideas to the distinc | . ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-12-Transformers/#4-comments",
    "relUrl": "/notes/2022-09-12-Transformers/#4-comments"
  },"10": {
    "doc": "Transformers and HPC",
    "title": "Transformers and HPC",
    "content": "- [1. Background on Transformers](#1-background-on-transformers) - [1.1. Transformers vs. RNN's (Motivation 1)](#11-transformers-vs-rnns-motivation-1) - [1.2. Useful info](#12-useful-info) - [1.2.1. Softmax](#121-softmax) - [2. Workflow for AAYN model](#2-workflow-for-aayn-model) - [3. Transformer architectures](#3-transformer-architectures) - [4. Comments](#4-comments) ",
    "url": "http://localhost:4000/blog-just-the-docs/notes/2022-09-12-Transformers/",
    "relUrl": "/notes/2022-09-12-Transformers/"
  }
}
